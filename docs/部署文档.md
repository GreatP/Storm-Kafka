# 部署文档

## 0 简介

### 0.1 项目目标

**实时** **工业** **大数据** **分析**

### 0.2 系统简介

系统版本:

    OS:        CentOS 7 1511 版
    Python:    2.7.5 (CentOS 7 自带)
    Java:      1.8.0_65 (CentOS 7 自带)
    Zookeeper: 3.4.9
    Storm:     1.0.2
    Scala:     2.11.8
    Kafka:     0.9.0.1
    Maven:     3.3.9

IP 及 端口 分配:

    Zookeeper: 192.168.1.1 192.168.1.2 192.168.1.3 开放端口: 2181 2888 3888
    Storm: Nimbus: 192.168.1.4 192.168.1.5 开放端口: 3772 3773 3774 6627 6699 8000 8080 6700 6701 6702 6703
           Supervisor: 192.168.1.6 192.168.1.7 192.168.1.8 192.168.1.9 开放端口: 3772 3773 3774 6627 6699 8000 8080 6700 6701 6702 6703
    Kafka: Broker: 192.168.1.10 192.168.1.11 192.168.1.12 开放端口: 9000 9092 9999

## 1 Zookeeper

### 1.1 简介

[Apache Zookeeper](http://zookeeper.apache.org/) 是 Hadoop 的一个子项目, 是一个致力于开发和管理开源服务器, 并且能实现高可靠性的分布式协调框架. 它包含一个简单的原语集, 分布式应用程序可以基于它实现同步服务, 配置维护和命名服务等.

Zookeeper 保证 2n + 1 台机器的集群最大允许 n 台机器挂掉而事务不中断.

### 1.2 搭建

#### 1.2.1 单机模式

此模式主要用于**开发人员本地环境下测试代码**

##### 1.2.1.1 解压 Zookeeper 并进入其根目录

```bash
tar -xzf zookeeper-3.4.9.tar.gz -C /usr/local/
cd /usr/local/zookeeper-3.4.9
```

##### 1.2.1.2 创建配置文件 conf/zoo.cfg

```bash
cp conf/zoo_sample.cfg conf/zoo.cfg
```

##### 1.2.1.3 修改内容如下:

```bash
tickTime=2000
initLimit=10
syncLimit=5
dataDir=/var/lib/zookeeper/data
dataLogDir=/var/lib/zookeeper/logs
clientPort=2181
```

* tickTime: 是 zookeeper 的最小时间单元的长度(以毫秒为单位), 它被用来设置心跳检测和会话最小超时时间(tickTime 的两倍)
* initLimit: 初始化连接时能容忍的最长 tickTime 个数
* syncLimit: follower 用于同步的最长 tickTime 个数
* dataDir: 服务器存储 **数据快照** 的目录
* dataLogDir: 服务器存储 **事务日志** 的目录
* clientPort: 用于 client 连接的 server 的端口

其中需要注意的是`dataDir`和`dataLogDir`, 分别是 zookeeper 运行时的数据目录和日志目录, 要保证 **这两个目录已创建** 且 **运行 zookeeper 的用户拥有这两个目录的所有权**

##### 1.2.1.4 测试

* 启动/关闭 Zookeeper:

```bash
bin/zkServer.sh start
bin/zkServer.sh stop
```

* 查看 Zookeeper 状态:

```bash
bin/zkServer.sh status
```

显示 `mode: standalone`, 单机模式.

* 使用 java 客户端连接 ZooKeeper

```bash
./bin/zkCli.sh -server 127.0.0.1:2181
```

然后就可以使用各种命令了, 跟文件操作命令很类似, 输入 help 可以看到所有命令.

#### 1.2.2 集群模式

此模式是 **生产环境中实际使用的模式**

因为 zookeeper 保证 2n + 1 台机器最大允许 n 台机器挂掉, 所以配置集群模式最好是奇数台机器: 3, 5, 7...

最少 3 台构成集群

##### 1.2.2.1 hosts 映射(可选)

```bash
echo "192.168.1.1 zoo1" >> /etc/hosts
echo "192.168.1.2 zoo2" >> /etc/hosts
echo "192.168.1.3 zoo3" >> /etc/hosts
```

##### 1.2.2.2 修改 zookeeper-3.4.9/conf/zoo.cfg 文件

```bash
tickTime=2000
initLimit=10
syncLimit=5
dataDir=/var/lib/zookeeper/data
dataLogDir=/var/lib/zookeeper/log
clientPort=2181
server.1=192.168.1.1:2888:3888
server.2=192.168.1.2:2888:3888
server.3=192.168.1.3:2888:3888
```

与单机模式的不同就是最后三条: `server.X=host:portA:portB`

```bash
server.1=192.168.1.1:2888:3888
server.2=192.168.1.2:2888:3888
server.3=192.168.1.3:2888:3888
```

或

```bash
server.1=zoo1:2888:3888
server.2=zoo2:2888:3888
server.3=zoo3:2888:3888
```

X 为标识为 X 的机器, host 为其 hostname 或 IP, portA 用于这台机器与集群中的 Leader 机器通信, portB 用于 server 选举 leader.

**PS**: 要配单机伪分布式的话, 可以修改这里为

```bash
server.1=localhost:2888:3888
server.2=localhost:2889:3889
server.3=localhost:2890:3890
```

然后每个 zookeeper 实例的 dataDir 和 dataLogDir 配置为不同的即可

##### 1.2.2.3 myid 文件

在标示为 X 的机器上, 将 X 写入 ${dataDir}/myid 文件, 如: 在 192.168.1.2 机器上的 /var/lib/zookeeper/data 目录下建立文件 myid, 写入 2

````bash
echo "2" > /var/lib/zookeeper/data/myid
````

##### 1.2.2.4 开放端口

CentOS 7 使用 firewalld 代替了原来的 iptables, 基本使用如下

```bash
systemctl start firewalld                                      # 启动防火墙
firewall-cmd --state                                           # 检查防火墙状态
firewall-cmd --zone=public --add-port=2888/tcp --permanent     # 永久开启 2888 端口
firewall-cmd --reload                                          # 重新加载防火墙规则
firewall-cmd --list-all                                        # 列出所有防火墙规则
```

把 Zookeeper 用到的端口开放出来

```bash
firewall-cmd --zone=public --add-port=2181/tcp --permanent     # 永久开启 2181 端口
firewall-cmd --zone=public --add-port=2888/tcp --permanent     # 永久开启 2888 端口
firewall-cmd --zone=public --add-port=3888/tcp --permanent     # 永久开启 3888 端口
firewall-cmd --reload                                          # 重新加载防火墙规则
```

##### 1.2.2.5 测试

* 在 **集群中所有机器上** 启动 zookeeper(尽量同时):

```bash
bin/zkServer.sh start
```

* 查看状态, 应该有一台机器显示`mode: leader`, 其余为`mode: follower`

```bash
bin/zkServer.sh status
```

* 使用 java 客户端连接 ZooKeeper

```bash
./bin/zkCli.sh -server 192.168.1.1:2181
```

然后就可以使用各种命令了, 跟文件操作命令很类似, 输入help可以看到所有命令.

* 关闭 zookeeper:

```bash
./bin/zkServer.sh stop
```

#### 1.2.3 Zookeeper 常见问题

查看状态时, 应该有一台机器显示`mode: leader`, 其余为`mode: follower`

```bash
bin/zkServer.sh status
```

当显示`Error contacting service. It is probably not running.`时, 可以查看日志

```bash
cat zookeeper.out
```

查看 zookeeper.out 日志可以看到是那些机器连不上, 可能是 **网络, ip, 端口, 配置文件, myid 文件** 的问题.
正常应该是: 先是一些 java 异常, 这是因为 ZooKeeper 集群启动的时候, 每个结点都试图去连接集群中的其它结点, 先启动的肯定连不上后面还没启动的, 所以上面日志前面部分的异常是可以忽略的, 当集群所有的机器的 zookeeper 都启动起来, 就没有异常了, 并选举出来了 leader.

**PS**: 因为 zkServer.sh 脚本中是用 nohup 命令启动 zookeeper 的, 所以 zookeeper.out 文件是在调用 zkServer.sh 时的路径下, 如:用 `bin/zkServer.sh start` 启动则 zookeeper.out 文件在 `zookeeper-3.4.9/` 下; 用 `zkServer.sh start` 启动则 zookeeper.out 文件在 `zookeeper-3.4.9/bin/` 下.

## 2 Storm

### 2.1 简介

[Apache Storm](http://storm.apache.org/): 分布式实时计算系统

与 Hadoop 的批处理相类似, Storm 可以对大量的数据流进行可靠的实时处理, 这一过程也称为"流式处理", 是分布式大数据处理的一个重要方向. Storm 支持多种类型的应用, 包括: 实时分析、在线机器学习、连续计算、分布式RPC(DRPC)、ETL等. Strom 的一个重要特点就是"快速"的数据处理, 有 benchmark 显示 Storm 能够达到单个节点每秒百万级 tuple 处理(Tuple 是 Storm 的最小数据单元)的速度. 快速的数据处理、优秀的可扩展性与容错性、便捷的可操作性与维护性、活跃的社区技术支持, 这就是 Storm.

Storm的适用场景:

1. 流数据处理. Storm 可以用来处理源源不断流进来的消息, 处理之后将结果写入到某个存储中去.
2. 分布式 rpc. 由于 Storm 的处理组件是分布式的, 而且处理延迟极低, 所以可以作为一个通用的分布式 rpc 框架来使用.

### 2.2 搭建

#### 2.2.1 单机模式

此模式主要用于 **开发人员本地环境下测试代码**

##### 2.2.1.1 搭建 Zookeeper (单机 or 集群)

见 [1.2.1 单机模式](# 1.2.1 单机模式) or 见 [1.2.2 集群模式](# 1.2.2 集群模式)

##### 2.2.1.2 安装 Storm 依赖库(Java、Python)

在集群中的所有机器上安装 Storm 必要的依赖组件: Java7(or 8), Python2.7.

使用 CentOS 7 自带的 Python 2.7.5 及 openjdk 1.8.0_65 即可

##### 2.2.1.3 解压 Storm 并启动 Storm 各个后台进程

不需额外配置, 即是单机模式

* **Nimbus**: 运行

```bash
nohup bin/storm nimbus > logs/nimbus-boot.log 2>&1 &
```

启动 Nimbus 后台程序, 并放到后台执行, 标准输出和错误输出定向到 `./logs/nimbus-boot.log`, 有问题时可以去看这个文件

* **Supervisor**: 运行

```bash
nohup bin/storm supervisor > logs/supervisor-boot.log 2>&1 &
```

启动 Supervisor 后台程序, 并放到后台执行, 标准输出和错误输出定向到 `./logs/supervisor-boot.log`, 有问题时可以去看这个文件

* **Storm UI**: 运行

```bash
nohup bin/storm ui > logs/ui-boot.log 2>&1 &
```

启动 Storm UI 后台程序, 并放到后台执行, 标准输出和错误输出定向到 `./logs/ui-boot.log`, 有问题时可以去看这个文件.

Storm UI 可以在浏览器中方便地监控集群与拓扑运行状况, 启动后可以通过 *http://{nimbus host}:8080* 观察集群的 Worker 资源使用情况、Topologies 的运行状态等信息.

**PS**: Storm 后台进程被启动后, 将在 Storm 安装部署目录下的 logs/ 子目录下生成各个进程的日志文件, 这是 Storm 的默认设置, 日志文件的路径与相关配置信息可以在 {STORM_HOME}/logback/cluster.xml 文件中修改.

#### 2.2.2 集群模式

此模式是 **生产环境中实际使用的模式**

##### 2.2.2.1 hosts 映射(可选)

最好配置主机名, 配置文件 conf/storm.yaml 中若是填写 IP, 在 Storm UI 中显示不正常

```bash
echo "192.168.1.4 nim1" >> /etc/hosts
echo "192.168.1.5 nim2" >> /etc/hosts

echo "192.168.1.6 sup1" >> /etc/hosts
echo "192.168.1.7 sup2" >> /etc/hosts
echo "192.168.1.8 sup3" >> /etc/hosts
echo "192.168.1.9 sup4" >> /etc/hosts
```

##### 2.2.2.2 搭建 Zookeeper 集群

见 [1.2.2 集群模式](# 1.2.2 集群模式)

关于 ZooKeeper 部署的两点说明:

*   ZooKeeper 必须在监控模式下运行. 因为 ZooKeeper 是个快速失败系统, 如果遇到了故障, ZooKeeper 服务会主动关闭. 更多详细信息请参考: http://zookeeper.apache.org/doc/r3.3.3/zookeeperAdmin.html#sc_supervision. 我们选择第一个方案: daemontools: http://cr.yp.to/daemontools.html
*   需要设置一个 cron 服务来定时压缩 ZooKeeper 的数据与事务日志. 因为 ZooKeeper 的后台进程不会处理这个问题, 如果不配置 cron, ZooKeeper 的日志会很快填满磁盘空间. 更多详细信息请参考: http://zookeeper.apache.org/doc/r3.3.3/zookeeperAdmin.html#sc_maintenance
    * 代替方案: http://nileader.blog.51cto.com/1381108/932156 第四种
    > 在 zoo.cfg 中配置的:
    > autopurge.purgeInterval 这个参数指定了清理频率, 单位是小时, 需要填写一个 1 或更大的整数, 默认是 0, 表示不开启自己清理功能.
    > autopurge.snapRetainCount 这个参数和上面的参数搭配使用, 这个参数指定了需要保留的快照数目. 默认是保留 3 个.

##### 2.2.2.3 安装 Storm 依赖库(Java、Python)

在集群中的所有机器上安装 Storm 必要的依赖组件: Java7(or 8), Python2.7.

使用 CentOS 7 自带的 Python 2.7.5 及 openjdk 1.8.0_65 即可

##### 2.2.2.4 解压 Storm 并进入其根目录

```bash
tar -xzf apache-storm-1.0.2.tar.gz -C /usr/local/
cd /usr/local/apache-storm-1.0.2
```

##### 2.2.2.5 修改 conf/storm.yaml 配置文件

storm.yaml 会覆盖 defaults.yaml 中各个配置项的默认值, 以下几个是在安装集群时必须配置的选项

```bash
########### These MUST be filled in for a storm configuration
# yaml 文件的配置使用"-"来表示数据的层次结构, 配置项的:后必须有空格, 否则该配置项无法识别
# Storm 关联的 ZooKeeper 集群的地址列表
storm.zookeeper.servers:
    - "192.168.1.1"
    - "192.168.1.2"
    - "192.168.1.3"

# 如果使用的 ZooKeeper 集群的端口不是默认端口, 还需要配置 storm.zookeeper.port
# storm.zookeeper.port: 2181

# Storm 工作目录, 需要提前创建该目录并给以足够的访问权限
storm.local.dir: "/var/lib/storm-workdir"

# 用作 nimbus 的机器的 host list, 若 nimbus 是单机, 可以使用 nimbus.seeds: ["nim1"], 这里用的双机
# 若是填写 IP, 在 Storm UI 中显示不正常
nimbus.seeds: ["nim1", "nim2"]

# Supervisor工作节点上 worker 的端口, 每个 worker 占用一个单独的端口用于接收消息, 有几个端口就最多会有几个 worker 运行, 这里配置了 4 个
supervisor.slots.ports:
    - 6700
    - 6701
    - 6702
    - 6703
```

##### 2.2.2.6 开放端口

把 Storm 用到的端口开放出来

```bash
firewall-cmd --zone=public --add-port=3772/tcp --permanent     # drpc.port
firewall-cmd --zone=public --add-port=3773/tcp --permanent     # drpc.invocations.port
firewall-cmd --zone=public --add-port=3774/tcp --permanent     # drpc.http.port
firewall-cmd --zone=public --add-port=6627/tcp --permanent     # nimbus.thrift.port
firewall-cmd --zone=public --add-port=6699/tcp --permanent     # pacemaker.port
firewall-cmd --zone=public --add-port=8000/tcp --permanent     # logviewer.port
firewall-cmd --zone=public --add-port=8080/tcp --permanent     # storm.ui.port
firewall-cmd --zone=public --add-port=6700/tcp --permanent     # supervisor.slots.ports -- 取决于上面的配置
firewall-cmd --zone=public --add-port=6701/tcp --permanent     # 
firewall-cmd --zone=public --add-port=6702/tcp --permanent     # 
firewall-cmd --zone=public --add-port=6703/tcp --permanent     # 
firewall-cmd --reload                                          # 重新加载防火墙规则
```
##### 2.2.2.7 启动 Storm 各个后台进程

和 Zookeeper 一样, Storm 也是快速失败(fail-fast)的系统, 这样 Storm 才能在任意时刻被停止, 并且当进程重启后被正确地恢复执行. 这也是为什么 Storm 不在进程内保存状态的原因, 即使 Nimbus 或 Supervisors 被重启, 运行中的 Topologies 不会受到影响. 以下是启动 Storm 各个后台进程的方式:

```bash
# 在 Storm 主控(Master)节点上运行 Nimbus 后台程序, 并放到后台执行, 标准输出和错误输出定向到 `./logs/nimbus-boot.log`, 有问题时可以去看这个文件
nohup bin/storm nimbus > logs/nimbus-boot.log 2>&1 &

# 在 Storm 各个工作节点(Worker)上运行 Supervisor 后台程序, 并放到后台执行, 标准输出和错误输出定向到 `./logs/supervisor-boot.log`, 有问题时可以去看这个文件
nohup bin/storm supervisor > logs/supervisor-boot.log 2>&1 &

# 在 Storm 主控(Master)节点上运行 Storm UI 后台程序, 并放到后台执行, 标准输出和错误输出定向到 `./logs/ui-boot.log`, 有问题时可以去看这个文件.
nohup bin/storm ui > logs/ui-boot.log 2>&1 &

# 在需要查看 work.log 节点上运行 Logviewer 后台程序, 并放到后台执行, 标准输出和错误输出定向到 `./logs/logviewer-boot.log`, 有问题时可以去看这个文件.
nohup bin/storm logviewer > logs/logviewer-boot.log 2>&1 &
```

Storm UI 可以在浏览器中方便地监控集群与拓扑运行状况, 启动后可以通过 *http://{nimbus host}:8080* 观察集群的 Worker 资源使用情况、Topologies 的运行状态等信息. Logviewer 是 Storm UI 中用来查看 Nimbus/Supervisor 的 log 的工具.

**PS**: Storm UI 必须在 Nimbus 机器(Nimbus 集群的话, 其中一台即可)上, 否则 UI 无法正常工作.

**Storm 后台进程被启动后, 将在 Storm 安装部署目录下的 logs/ 子目录下生成各个进程的日志文件, 这是 Storm 的默认设置, 日志文件的路径与相关配置信息可以在 {STORM_HOME}/logback/cluster.xml 文件中修改.**

至此, Storm 集群已经部署、配置完毕, 可以 向集群 提交拓扑 运行了.

##### 2.2.2.8 查看 Storm 状态

利用 Storm UI, 通过 http://{nimbus host}:8080 查看集群的各种状态.

##### 2.2.2.9 监控 Supervisor 的运行情况(可选)

Storm 提供了一种机制, 使 Supervisor 定期运行管理人员提供的脚本, 以确定节点是否正常.

管理人员可以让 Supervisor 执行位于 storm.health.check.dir 中的脚本来确定节点是否处于健康状态, 如果脚本检测到节点处于不正常状态, 则在标准输出中打印一行以 ERROR 开头的字符串.

Supervisor 将定期运行storm.health.check.dir 中的脚本并检查输出, 如果脚本的输出包含字符串 ERROR, Supervisor 将关闭所有工作线程并退出.

如果 Supervisor 正在运行, 可以调用"/bin/storm node-health-check"来确定节点是否正常.

在 conf/storm.yaml 配置 storm.health.check.dir:

```bash
storm.health.check.dir: " healthchecks"
```

配置执行 healthcheck 脚本的周期:

```bash
storm.health.check.timeout.ms: 5000
```

**PS**: **脚本必须具有执行权限.**

##### 2.2.2.10 配置外部库与环境变量(可选)

如果你需要使用某些外部库或者定制插件的功能, 你可以将相关 jar 包放入 extlib 与 extlib-daemon 目录下. 注意, extlib-daemon 目录仅用于存储后台进程(Nimbus, Supervisor, DRPC, UI, Logviewer)所需的 jar 包, 例如 HDFS 以及定制的调度库.

另外可以使用 STORM_EXT_CLASSPATH 和 STORM_EXT_CLASSPATH_DAEMON 两个环境变量来配置普通外部库与"仅用于后台进程"外部库的 classpath.

### 2.3 向 Storm 集群提交任务

* 启动 Storm Topology:

```bash
storm jar allmycode.jar org.me.MyTopology arg1 arg2 arg3
```

其中, allmycode.jar 是包含 Topology 实现代码的 jar 包, org.me.MyTopology 的 main 方法是 Topology 的入口, arg1、arg2 和 arg3 为 org.me.MyTopology 执行时需要传入的参数.

* 停止 Storm Topology:

```bash
storm kill {toponame}
```

其中, {toponame} 为 Topology 提交到 Storm 集群时指定的 Topology 任务名称.

### 2.4 Storm 常见配置问题

* 运行 storm 命令报错

出现语法错误:

```
File "/home/storm/apache-storm-0.9.3/bin/storm", line 61
   normclasspath = cygpath if sys.platform == 'cygwin' else identity
                            ^
SyntaxError: invalid syntax

```

这是由于系统中安装的低版本 Python 部分语法不支持, 需要重新安装高版本 Python(如2.7.x).

**PS**: 部分系统Python默认安装位置不是 `/usr/bin/python`, 必须在 Python 安装完成之后将安装版本Python关联到该位置. 参考操作方法: `cd /usr/bin` `mv python python.bk``ln -s /usr/local/Python-2.7.8/python python`

* Storm 在 ssh 断开后自动关闭

这是由于 Storm 是由默认的 Shell 机制打开运行, 在 ssh 或 telnet 断开后终端会将挂断信号发送到控制进程, 进而会关闭该 Shell 进程组中的所有进程. 因此需要在 Storm 后台启动时使用 `nohup` 命令和 `&` 标记可以使进程忽略挂断信号, 避免程序的异常退出:

```bash
nohup bin/storm nimbus > logs/nimbus-boot.log 2>&1 &
nohup bin/storm supervisor > logs/supervisor-boot.log 2>&1 &
nohup bin/storm ui > logs/ui-boot.log 2>&1 &
nohup bin/storm logviewer > logs/logviewer-boot.log 2>&1 &
```

* Storm UI 网页无法打开

检查 Storm 主机(nimbus 与 ui 所在运行服务器)的防火墙设置, 是否存在监控端口屏蔽(ui 的默认端口是 8080)

**PS**: 测试环境下可以不考虑安全问题直接关闭防火墙

* Strom UI 网页中没有 topology 信息

只有集群(Cluster)模式的 topology 才会在监控页面显示, 需要将提交到集群的 topology 的运行模式由本地模式(local mode)改为集群模式

* Storm UI 网页中无法打开各个端口的 worker.log

在需要查看 log 的机器上启动 logviewer 进程:

```bash
nohup bin/storm logviewer > logs/logviewer-boot.log 2>&1 &
```

* expected '<document start>', but found BlockMappingStart 错误

Storm 启动失败, 在 nohup.out 中有如下错误信息

```
Exception in thread "main" expected '<document start>', but found BlockMappingStart
```

一般在这类信息后会有相关错误位置说明信息, 如

```
in 'reader', line 23, column 2:
     nimbus.host: "hd124"
     ^
```

或者

```
in 'reader', line 7, column 1:
     storm.zookeeper.port: 2181
     ^
```

这类错误主要是storm.yaml文件的配置格式错误造成的, 一般是配置项的空格遗漏问题. 如上面两例分别表示nimbus.host与storm.zookeeper.port两个配置项开头缺少空格, 或者":"后缺少空格. 正确添加空格后重新启动Storm即可.

* Storm worker 数量与配置数量不一致

在 topology 中设置 worker 数量:

`conf.setNumWorkers(6);`

但是, 集群中实际的 worker 数量却不到6.

这是由于每个 supervisor 中有 worker 数量的上限, 这个上限值除了要满足系统允许的最大 slot 上限值 `8` 之外, 还需要小于 Storm 配置文件中的端口数量:

```
supervisor.slots.ports:
    - 6700
    - 6701
    - 6702
    - 6703
```

例如这里 supervisor 只配置了 4 个端口, 那么在这个 supervisor 上最多只能运行 4 个 worker 进程. 因此, 如果需要更多的 worker 就需要配置更多的端口.

* 日志无法记录到程序中配置的路径

Storm 默认将日志统一记录到 `$STORM_HOME/logs` 目录中, 不支持在程序中自定义的路径. 但是, 集群的日志记录目录是可以修改的, 0.9 以上版本的 Storm 可以在 `$STORM_HOME/logback/cluster.xml` 配置文件中修改, 其他早期版本可以在`log4j/*.properties` 配置文件中修改.

## 3 Kafka

### 3.1 简介

[Apache Kafka](http://kafka.apache.org/)是一个分布式消息发布订阅系统. Kafka 系统快速、可扩展并且可持久化. 它的分区特性, 可复制和可容错都是其不错的特性.

### 3.2 搭建

#### 3.2.1 hosts 映射(可选, 建议)

```bash
echo "192.168.1.10 kfk1" >> /etc/hosts
echo "192.168.1.11 kfk2" >> /etc/hosts
echo "192.168.1.12 kfk3" >> /etc/hosts
```

#### 3.2.2 搭建 Zookeeper (单机 or 集群)

Broker, Producer, Consumer 的运行都需要 ZooKeeper

见 [1.2.1 单机模式](# 1.2.1 单机模式) or 见 [1.2.2 集群模式](# 1.2.2 集群模式)

#### 3.2.3 Broker 的配置

```bash
tar -xzf kafka_2.11-0.9.0.1.tgz -C /usr/local/
cd /usr/local/kafka_2.11-0.9.0.1
```

config 文件夹下是各个组件的配置文件, server.properties 是 Broker 的配置文件, 需要修改的有

```bash
######################### Server Basics #########################
broker.id=0                    # 本 Broker 的 id, 只要非负数且各 Broker 的 id 不同即可, 一般依次加 1

##################### Socket Server Settings #####################
listeners=PLAINTEXT://:9092    # Broker 监听的端口, Producer, Consumer 会连接这个端口
port=9092                      # 同上
host.name=kfk1                 # 本 Broker 的 hostname

########################## Topic Basics ##########################
delete.topic.enable=true       # 配置为可以使用 delete topic 命令

########################### Log Basics ###########################
log.dirs=/var/lib/kafka-logs   # log 目录, 此目录要存在且有足够权限

###################### Log Retention Policy ######################
log.roll.hours=2               # 开始一个新的 log 文件片段的最大时间
log.retention.hours=24         # 控制一个 log 文件保留多长个小时
log.retention.bytes=1073741824 # 所有 log 文件的最大大小
log.segment.bytes=104857600    # 单一的 log 文件最大大小
log.cleanup.policy=delete      # log 清除策略
log.retention.check.interval.ms=60000

############################ Zookeeper ############################
zookeeper.connect=zoo1:2181,zoo2:2181,zoo3:2181 # Zookeeper 的连接信息
```

**注意: broker.id 和 host.name 在每台机器上是不一样的, 要按实际填写**

即在 kfk2, kfk3 上

```bash
broker.id=1
host.name=kfk2
```

```bash
broker.id=2
host.name=kfk3
```

**PS**: 在 kafka 安装目录下的 `./site-docs` 目录下有 `kafka_config.html`, `producer_config.html`, `consumer_config.html` 三个文件, 分别讲解 broker, producer, consumer 配置参数含义.

#### 3.2.4 开放端口

把 Kafka 用到的端口开放出来

```bash
firewall-cmd --zone=public --add-port=9000/tcp --permanent     # 永久开启 9000 端口(kafka manager)
firewall-cmd --zone=public --add-port=9092/tcp --permanent     # 永久开启 9092 端口(brokers)
firewall-cmd --zone=public --add-port=9999/tcp --permanent     # 永久开启 9999 端口(JMX)
firewall-cmd --reload                                          # 重新加载防火墙规则
```

#### 3.2.5 Broker 运行与终止

Broker 运行与终止命令如下. 运行时将 Broker 放到后台执行, 且不受终端关闭的影响, 标准输出和错误输出定向到 `./logs/kafka-server-boot.log`, 有问题时可以去看这个文件

```bash
# 运行
nohup bin/kafka-server-start.sh config/server.properties > logs/kafka-server-boot.log 2>&1 &

# 终止
bin/kafka-server-stop.sh config/server.properties
```

#### 3.2.6 测试

我们使用 Kafka 自带的基于 终端 的 Producer 和 Consumer 脚本做测试.

先只启动一台机器上的 Broker. 在 kfk1 上运行

```bash
nohup bin/kafka-server-start.sh config/server.properties > logs/kafka-server-boot.log 2>&1 &
```

##### 1. 创建 Topic

创建一个 名为"TestCase"的 单分区 单副本 的 Topic.

```
bin/kafka-topics.sh --create --zookeeper localhost:2181 --topic TestCase --replication-factor 1 --partitions 1
```

查看有哪些 Topic:

```
$ bin/kafka-topics.sh --list --zookeeper localhost:2181
TestCase
```

运行`describe topics`命令, 可以知道 Topic 的具体分配:

```
$ bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test
Topic:test    PartitionCount:1    ReplicationFactor:1    Configs:
    Topic: test    Partition: 0    Leader: 0    Replicas: 0    Isr: 0
```

解释一下输出的内容. 第一行给出了所有 partition 的一个摘要, 每行给出一个 partition 的信息. 因为我们这个 topic 只有一个 partition 所以只有一行信息.

* "leader" 负责所有 partition 的读和写请求的响应. "leader" 是随机选定的.
* "replicas" 是备份节点列表, 包含所有复制了此 partition log 的节点, 不管这个节点是否为 leader 也不管这个节点当前是否存活, 只是显示.
* "isr" 是当前处于同步状态的备份节点列表. 即 "replicas" 列表中处于存活状态并且与 leader 一致的节点.

可以发现这个 Topic 没有副本而且它在 [我们创建它时集群仅有的一个节点] Broker 0 上.

另外, 除去手工创建 Topic 以外, 你也可以将你的 Brokers 配置成当消息发布到一个不存在的 Topic 时自动创建此 Topic.

##### 2. 启动 生产者

Kafka 附带一个 **终端生产者** 可以从文件或者标准输入中读取输入然后发送这个消息到 Kafka 集群. 默认情况下每行信息被当做一个消息发送.

运行生产者脚本然后在终端中输入一些消息, 即可发送到 Broker.

```
$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic TestCase
This is a message
This is another message
```

**PS**: 通过键入 **Ctrl-C** 来终止终端生产者.

##### 3. 启动 消费者

Kafka 也附带了一个 **终端生产者** 可以导出这些消息到标准输出.

```
$ bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic TestCase --from-beginning
This is a message
This is another message
```

`--from-beginning` 参数使得可以接收到 topic 的所有消息, 包括 consumer 启动前的. 去掉后则为仅接收 consumer 启动后 kafka 收到的消息.

如果你在不同的终端运行生产者和消费者这两个命令, 那么现在你就应该能再生产者的终端中键入消息同时在消费者的终端中看到.

所有的命令行工具都有很多可选的参数; 不添加参数直接执行这些命令将会显示它们的使用方法, 更多内容可以参考他们的手册.

**PS**: 通过键入 **Ctrl-C** 来终止终端消费者.

##### 4. 配置一个多节点集群

我们已经成功的以单 Broker 的模式运行起来了, 但这并没有意思. 对于 Kafka 来说, 一个单独的 Broker 就是一个大小为 1 的集群, 所以集群模式就是多启动几个 Broker 实例.

我们将我们的集群扩展到3个节点. 在另外两台机器 kfk2, kfk3 上运行

```bash
nohup bin/kafka-server-start.sh config/server.properties > logs/kafka-server-boot.log 2>&1 &
```

现在我们可以创建一个新的 Topic 并制定副本数量为 3:

```
bin/kafka-topics.sh --create --zookeeper localhost:2181 --topic my-replicated-topic --replication-factor 3 --partitions 1
```

运行`describe topics`命令, 可以知道每个 Broker 具体的工作:

```
$ bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic
Topic:my-replicated-topic    PartitionCount:1    ReplicationFactor:3    Configs:
    Topic: my-replicated-topic    Partition: 0    Leader: 1    Replicas: 1,2,0    Isr: 1,2,0
```

注意本例中 Broker 1 是这个有一个 partition 的 topic 的 leader.

现在我们发布几个消息到我们的新 topic 上:

```
$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic my-replicated-topic
my test message 1
my test message 2
```

现在让我们消费这几个消息:

```
$ bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic my-replicated-topic --from-beginning
my test message 1
my test message 2
```

现在让我们测试一下集群容错. Broker 1 正在作为 leader, 所以我们杀掉它:

```
$ ps | grep server.properties
...
7564 ttys002    0:15.91 /System/Library/Frameworks/JavaVM.framework/Versions/1.8/Home/bin/java...
...

$ kill -9 7564
```

或在 kfk1 机器上运行

```bash
bin/kafka-server-stop.sh config/server.properties
```

此时, 集群领导已经切换到一个从服务器上, Broker 1 节点也不在出现在同步副本列表中了:

```
$ bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic
Topic:my-replicated-topic    PartitionCount:1    ReplicationFactor:3    Configs:
    Topic: my-replicated-topic    Partition: 0    Leader: 2    Replicas: 1,2,0    Isr: 2,0
```

而且现在消息的消费仍然能正常进行, 即使原来负责写的节点已经失效了.

```
$ bin/kafka-console-consumer.sh --zookeeper localhost:2181 --from-beginning --topic my-replicated-topic
...
my test message 1
my test message 2
```

##### 5. 使用 Kafka Connect 进行数据导入导出

从终端写入数据, 数据也写回终端是默认的. 但是你可能希望从一些其它的数据源或者导出 Kafka 的数据到其它的系统. 相比其它系统需要自己编写集成代码, 你可以直接使用Kafka的 Connect 直接导入或者导出数据. Kafka Connect 是 Kafka 自带的用于数据导入和导出的工具. 它是一个扩展的可运行连接器(runs*connectors*)工具, 可实现自定义的逻辑来实现与外部系统的集成交互. 在这个快速入门中我们将介绍如何通过一个简单的从文本导入数据、导出数据到文本的连接器来调用 Kafka Connect. 首先我们从创建一些测试的基础数据开始:

```
echo -e "foo\nbar" > test.txt
```

接下来我们采用*standalone*模式启动两个 connectors, 也就是让它们都运行在独立的、本地的、不同的进程中. 我们提供三个参数化的配置文件, 第一个提供共有的配置用于 Kafka Connect 处理, 包含共有的配置比如连接哪个 Kafka broker 和数据的序列化格式. 剩下的配置文件制定每个 connector 创建的特定信息. 这些文件包括唯一的 connector 的名字, connector 要实例化的类和其它的一些 connector 必备的配置.

```
bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties
```

上述简单的配置文件已经被包含在 Kafka 的发行包中, 它们将使用默认的之前我们启动的本地集群配置创建两个 connector: 第一个作为源 connector 从一个文件中读取每行数据然后将他们发送 Kafka 的 topic, 第二个是一个输出(sink)connector 从 Kafka 的 topic 读取消息, 然后将它们输出成输出文件的一行行的数据. 在启动的过程你讲看到一些日志消息, 包括一些提示 connector 正在被实例化的信息. 一旦 Kafka Connect 进程启动以后, 源 connector 应该开始从 `test.txt` 中读取数据行, 并将他们发送到 topic `connect-test` 上, 然后输出 connector 将会开始从 topic 读取消息然后把它们写入到 `test.sink.txt` 中.

我们可以查看输出文件来验证通过整个管线投递的数据:

```
$ cat test.sink.txt
foo
bar
```

注意这些数据已经被保存到了 Kafka 的 `connect-test` topic 中, 所以我们还可以运行一个终端消费者来看到这些数据(或者使用自定义的消费者代码来处理数据):

```
$ bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic connect-test --from-beginning
{"schema":{"type":"string","optional":false},"payload":"foo"}
{"schema":{"type":"string","optional":false},"payload":"bar"}
...
```

connector 在持续的处理着数据, 所以我们可以向文件中添加数据然后观察到它在这个管线中的传递:

```
echo "Another line" >> test.txt
```

你应该可以观察到新的数据行出现在终端消费者中和输出文件中.

##### 6. 使用 Kafka Streams 来处理数据

Kafka Streams 是一个用来对 Kafka brokers 中保存的数据进行实时处理和分析的客户端库. 这个入门示例将演示如何启动一个采用此类库实现的流处理程序. 下面是 `WordCountDemo` 示例代码的 GIST(为了方便阅读已经转化成了 Java 8 的 lambda 表达式).

```
KTable wordCounts = textLines
    // 按照空格将每个文本行拆分成单词
    .flatMapValues(value -> Arrays.asList(value.toLowerCase().split("\\W+")))
    // 确保每个单词作为记录的 key 值以便于下一步的聚合
    .map((key, value) -> new KeyValue<>(value, value))
    // 计算每个单词的出现频率并将他们保存到 "Counts" 的表中
    .countByKey("Counts")
```

上述代码实现了计算每个单词出现频率直方图的单词计数算法. 但是它与之前常见的操作有限数据的示例相比有明显的不同, 它被设计成一个操作 **无边界限制的流数据** 的程序. 与有界算法相似它是一个有状态算法, 它可以跟踪并更新单词的计数. 但是它必须支持处理无边界限制的数据输入的假设, 它将在处理数据的过程持续的输出自身的状态和结果, 因为它不能明确的知道合适已经完成了所有输入数据的处理.

接下来我们准备一些发送到 Kafka topic 的输入数据, 随后它们将被 Kafka Streams 程序处理.

```
echo -e "all streams lead to kafka\nhello kafka streams\njoin kafka summit" > file-input.txt
```

接下来我们使用终端生产者发送这些输入数据到名为 **streams-file-input** 的输入 topic (在实际应用中, 流数据会是不断流入处理程序启动和运行用的Kafka):

```
bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic streams-file-input
```

```
cat file-input.txt | bin/kafka-console-producer.sh --broker-list localhost:9092 --topic streams-file-input
```

现在我们可以启动WordCount示例程序来处理这些数据了:

```
bin/kafka-run-class.sh org.apache.kafka.streams.examples.wordcount.WordCountDemo
```

在 STDOUT 终端不会有任何日志输出, 因为所有的结果被不断的写回了另外一个名为 **streams-wordcount-output** 的 topic 上. 这个实例将会运行一会儿, 之后与典型的流处理程序不同它将会自动退出.

现在我们可以通过读取这个单词计数示例程序的输出 topic 来验证结果:

```
bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic streams-wordcount-output --from-beginning \
            --formatter kafka.tools.DefaultMessageFormatter \
            --property print.key=true \
            --property print.value=true \
            --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \
            --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer
```

以下输出数据将会被打印到终端上:

```
all     1
streams 1
lead    1
to      1
kafka   1
hello   1
kafka   2
streams 2
join    1
kafka   3
summit  1
```

可以看到, 第一列是 Kafka 的消息的健, 第二列是这个消息的值, 他们都是 `java.lang.String` 格式的. 注意这个输出结果实际上是一个持续更新的流, 每一行(例如、上述原始输出的每一行)是一个单词更新之后的计数. 对于 key 相同的多行记录, 每行都是前面一行的更新.

现在你可以向 **streams-file-input** topic写入更多的消息并观察 **streams-wordcount-output** topic表述更新单词计数的新的消息.

## 4 最后

* 因为上面这三个框架都是 Java 的, 所以可以调整 Java 堆大小以优化上面这些程序的运行. Java 堆太小会导致程序难以运行; Java 堆太大(超出物理内存)会导致程序被交换到磁盘, 性能急剧降低. 例如: 4 G 内存的专用服务器可以分配 3 G 的 Java 堆, 最好的建议是运行负载测试, 然后确保远低于会导致系统交换的堆大小.

## 附录 A kafka 部分配置参数翻译

**参数说明**

**boker 部分参数说明** (配置文件位于 config/server.properties)

| name                            | 默认值                              | 描述                                       |
| ------------------------------- | -------------------------------- | ---------------------------------------- |
| broker.id                       | none                             | 每一个boker都有一个唯一的id作为它们的名字。 这就允许boker切换到别的主机/端口上,  consumer依然知道 |
| enable.zookeeper                | true                             | 允许注册到zookeeper                           |
| log.flush.interval.messages     | Long.MaxValue                    | 在数据被写入到硬盘和消费者可用前最大累积的消息的数量               |
| log.flush.interval.ms           | Long.MaxValue                    | 在数据被写入到硬盘前的最大时间                          |
| log.flush.scheduler.interval.ms | Long.MaxValue                    | 检查数据是否要写入到硬盘的时间间隔。                       |
| log.retention.hours             | 168                              | 控制一个log保留多长个小时                           |
| log.retention.bytes             | -1                               | 控制log文件最大尺寸                              |
| log.cleaner.enable              | false                            | 是否log cleaning                           |
| log.cleanup.policy              | delete                           | delete还是compat. 其它控制参数还包括log.cleaner.threads, log.cleaner.io.max.bytes.per.second, log.cleaner.dedupe.buffer.size, log.cleaner.io.buffer.size, log.cleaner.io.buffer.load.factor, log.cleaner.backoff.ms, log.cleaner.min.cleanable.ratio, log.cleaner.delete.retention.ms |
| log.dir                         | /tmp/kafka-logs                  | 指定log文件的根目录                              |
| log.segment.bytes               | 1*1024*1024*1024                 | 单一的log segment文件大小                       |
| log.roll.hours                  | 24 * 7                           | 开始一个新的log文件片段的最大时间                       |
| message.max.bytes               | 1000000 + MessageSet.LogOverhead | 一个socket 请求的最大字节数                        |
| num.network.threads             | 3                                | 处理网络请求的线程数                               |
| num.io.threads                  | 8                                | 处理IO的线程数                                 |
| background.threads              | 10                               | 后台线程序                                    |
| num.partitions                  | 1                                | 默认分区数                                    |
| socket.send.buffer.bytes        | 102400                           | socket SO_SNDBUFF参数                      |
| socket.receive.buffer.bytes     | 102400                           | socket SO_RCVBUFF参数                      |
| zookeeper.connect               | localhost:2182/kafka             | 指定zookeeper连接字符串,  格式如hostname:port/chroot。chroot是一个namespace |
| zookeeper.connection.timeout.ms | 6000                             | 指定客户端连接zookeeper的最大超时时间                  |
| zookeeper.session.timeout.ms    | 6000                             | 连接zk的session超时时间                         |
| zookeeper.sync.time.ms          | 2000                             | zk follower落后于zk leader的最长时间             |

**producer参数说明(配置文件位于config/producer.properties或者在程序内定义)**

```
    # 指定kafka节点列表, 用于获取metadata, 不必全部指定
    metadata.broker.list=192.168.2.105:9092,192.168.2.106:9092

    # 指定分区处理类。默认kafka.producer.DefaultPartitioner, 表通过key哈希到对应分区
    #partitioner.class=com.meituan.mafka.client.producer.CustomizePartitioner

    # 是否压缩, 默认0表示不压缩, 1表示用gzip压缩, 2表示用snappy压缩。压缩后消息中会有头来指明消息压缩类型, 故在消费者端消息解压是透明的无需指定。
    compression.codec=none
      
    # 指定序列化处理类(mafka client API调用说明-->3.序列化约定wiki), 默认为kafka.serializer.DefaultEncoder,即byte[]
    serializer.class=com.meituan.mafka.client.codec.MafkaMessageEncoder
    # serializer.class=kafka.serializer.DefaultEncoder
    # serializer.class=kafka.serializer.StringEncoder

    # 如果要压缩消息, 这里指定哪些topic要压缩消息, 默认empty, 表示不压缩。
    #compressed.topics=

    ########### request ack ###############
    # producer接收消息ack的时机.默认为0.
    # 0: producer不会等待broker发送ack
    # 1: 当leader接收到消息之后发送ack
    # 2: 当所有的follower都同步消息成功后发送ack.
    request.required.acks=0

    # 在向producer发送ack之前,broker允许等待的最大时间
    # 如果超时,broker将会向producer发送一个error ACK.意味着上一次消息因为某种
    # 原因未能成功(比如follower未能同步成功)
    request.timeout.ms=10000
    ########## end #####################

    # 同步还是异步发送消息, 默认“sync”表同步, "async"表异步。异步可以提高发送吞吐量,
    # 也意味着消息将会在本地buffer中,并适时批量发送, 但是也可能导致丢失未发送过去的消息
    producer.type=sync

    ############## 异步发送 (以下四个异步参数可选) ####################
    # 在async模式下,当message被缓存的时间超过此值后,将会批量发送给broker,默认为5000ms
    # 此值和batch.num.messages协同工作.
    queue.buffering.max.ms = 5000

    # 在async模式下,producer端允许buffer的最大消息量
    # 无论如何,producer都无法尽快的将消息发送给broker,从而导致消息在producer端大量沉积
    # 此时,如果消息的条数达到阀值,将会导致producer端阻塞或者消息被抛弃, 默认为10000
    queue.buffering.max.messages=20000

    # 如果是异步, 指定每次批量发送数据量, 默认为200
    batch.num.messages=500

    # 当消息在producer端沉积的条数达到"queue.buffering.max.meesages"后
    # 阻塞一定时间后,队列仍然没有enqueue(producer仍然没有发送出任何消息)
    # 此时producer可以继续阻塞或者将消息抛弃,此timeout值用于控制"阻塞"的时间
    # -1: 无阻塞超时限制,消息不会被抛弃
    # 0:立即清空队列,消息被抛弃
    queue.enqueue.timeout.ms=-1
    ################ end ###############

    # 当producer接收到error ACK,或者没有接收到ACK时,允许消息重发的次数
    # 因为broker并没有完整的机制来避免消息重复,所以当网络异常时(比如ACK丢失)
    # 有可能导致broker接收到重复的消息,默认值为3.
    message.send.max.retries=3

    # producer刷新topic metada的时间间隔,producer需要知道partition leader的位置,以及当前topic的情况
    # 因此producer需要一个机制来获取最新的metadata,当producer遇到特定错误时,将会立即刷新
    # (比如topic失效,partition丢失,leader失效等),此外也可以通过此参数来配置额外的刷新机制, 默认值600000
    topic.metadata.refresh.interval.ms=60000
```

**consumer参数说明(配置文件位于config/consumer.properties或者在程序内定义)**

```
    # zookeeper连接服务器地址, 此处为线下测试环境配置(kafka消息服务-->kafka broker集群线上部署环境wiki)
    # 配置例子："127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002"
    zookeeper.connect=192.168.2.225:2181,192.168.2.225:2182,192.168.2.225:2183/config/mobile/mq/mafka

    # zookeeper的session过期时间, 默认5000ms, 用于检测消费者是否挂掉, 当消费者挂掉, 其他消费者要等该指定时间才能检查到并且触发重新负载均衡
    zookeeper.session.timeout.ms=5000
    zookeeper.connection.timeout.ms=10000

    # 指定多久消费者更新offset到zookeeper中。注意offset更新时基于time而不是每次获得的消息。一旦在更新zookeeper发生异常并重启, 将可能拿到已拿到过的消息
    zookeeper.sync.time.ms=2000

    #指定消费组
    group.id=xxx

    # 当consumer消费一定量的消息之后,将会自动向zookeeper提交offset信息
    # 注意offset信息并不是每消费一次消息就向zk提交一次,而是现在本地保存(内存),并定期提交,默认为true
    auto.commit.enable=true

    # 自动更新时间。默认60 * 1000
    auto.commit.interval.ms=1000

    # 当前consumer的标识,可以设定,也可以有系统生成,主要用来跟踪消息消费情况,便于观察
    conusmer.id=xxx

    # 消费者客户端编号, 用于区分不同客户端, 默认客户端程序自动产生
    client.id=xxxx

    # 最大取多少块缓存到消费者(默认10)
    queued.max.message.chunks=50

    # 当有新的consumer加入到group时,将会reblance,此后将会有partitions的消费端迁移到新
    # 的consumer上,如果一个consumer获得了某个partition的消费权限,那么它将会向zk注册
    # "Partition Owner registry"节点信息,但是有可能此时旧的consumer尚没有释放此节点,
    # 此值用于控制,注册节点的重试次数.
    rebalance.max.retries=5

    # 获取消息的最大尺寸,broker不会像consumer输出大于此值的消息chunk
    # 每次feth将得到多条消息,此值为总大小,提升此值,将会消耗更多的consumer端内存
    fetch.min.bytes=6553600

    # 当消息的尺寸不足时,server阻塞的时间,如果超时,消息将立即发送给consumer
    fetch.wait.max.ms=5000
    socket.receive.buffer.bytes=655360

    # 如果zookeeper没有offset值或offset值超出范围。那么就给个初始的offset。有smallest、largest、
    # anything可选, 分别表示给当前最小的offset、当前最大的offset、抛异常。默认largest
    auto.offset.reset=smallest

    # 指定序列化处理类(mafka client API调用说明-->3.序列化约定wiki), 默认为kafka.serializer.DefaultDecoder,即byte[]
    derializer.class=com.meituan.mafka.client.codec.MafkaMessageDecoder
```
