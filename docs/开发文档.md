# 开发文档

## 1 Zookeeper

### 1.1 简介

Apache Zookeeper 是 Hadoop 的一个子项目, 是一个致力于开发和管理开源服务器, 并且能实现高可靠性的分布式协调框架. 它包含一个简单的原语集, 分布式应用程序可以基于它实现同步服务, 配置维护和命名服务等.

Zookeeper 保证 2n + 1 台机器的集群最大允许 n 台机器挂掉而事务不中断.

#### 1.1.1 角色

Zookeeper 中的角色主要有以下三类, 如下表所示:

![zoo角色](zoo角色.jpg "zoo角色")

系统模型如图所示:

![zoo模型](zoo模型.jpg "zoo模型")

#### 1.1.2 ZooKeeper 的工作原理

详见: http://ifeve.com/zookeeperover/

#### 1.2.3 Zookeeper 常见问题

查看状态时, 应该有一台机器显示`mode: leader`, 其余为`mode: follower`

```bash
bin/zkServer.sh status
```

当显示`Error contacting service. It is probably not running.`时, 可以查看日志

```bash
cat zookeeper.out
```

查看 zookeeper.out 日志可以看到是那些机器连不上, 可能是 **网络, ip, 端口, 配置文件, myid 文件** 的问题.
正常应该是: 先是一些 java 异常, 这是因为 ZooKeeper 集群启动的时候, 每个结点都试图去连接集群中的其它结点, 先启动的肯定连不上后面还没启动的, 所以上面日志前面部分的异常是可以忽略的, 当集群所有的机器的 zookeeper 都启动起来, 就没有异常了, 并选举出来了 leader.

PS: 因为 zkServer.sh 脚本中是用 nohup 命令启动 zookeeper 的, 所以 zookeeper.out 文件是在调用 zkServer.sh 时的路径下, 如:用 `bin/zkServer.sh start` 启动则 zookeeper.out 文件在 `zookeeper-3.4.9/` 下; 用 `zkServer.sh start` 启动则 zookeeper.out 文件在 `zookeeper-3.4.9/bin/` 下.

## 2 Storm

### 2.1 简介

Storm: 分布式实时计算系统

与 Hadoop 的批处理相类似, Storm 可以对大量的数据流进行可靠的实时处理, 这一过程也称为"流式处理", 是分布式大数据处理的一个重要方向. Storm 支持多种类型的应用, 包括: 实时分析、在线机器学习、连续计算、分布式RPC(DRPC)、ETL等. Strom 的一个重要特点就是"快速"的数据处理, 有 benchmark 显示 Storm 能够达到单个节点每秒百万级 tuple 处理(Tuple 是 Storm 的最小数据单元)的速度. 快速的数据处理、优秀的可扩展性与容错性、便捷的可操作性与维护性、活跃的社区技术支持, 这就是 Storm.

Storm的适用场景:

1. 流数据处理. Storm 可以用来处理源源不断流进来的消息, 处理之后将结果写入到某个存储中去.
2. 分布式 rpc. 由于 Storm 的处理组件是分布式的, 而且处理延迟极低, 所以可以作为一个通用的分布式 rpc 框架来使用.

#### 2.1.1 Storm 的基本概念

1. Topology: Storm 中运行的一个实时应用程序, 因为 **各个组件间的消息流动** 形成 **逻辑上的一个拓扑结构**.
2. Spout: 在一个 Topology 中产生源数据流的组件, 读取原始数据, 为 Bolt 提供数据. 通常情况下 Spout 会从外部数据源中读取数据, 然后转换为 Topology 内部的源数据. Spout 是一个主动的角色, 其接口中有个 nextTuple() 函数, Storm 框架会不停地调用此函数, 用户只要在其中生成源数据即可.
3. Bolt: 在一个 Topology 中接受数据然后执行处理的组件, 从 Spout 或其它 Bolt 接收数据, 并处理数据, 处理结果可作为其它 Bolt 的数据源或最终结果. Bolt 可以执行过滤、函数操作、合并、写数据库等任何操作. Bolt 是一个被动的角色, 其接口中有个 execute(Tuple input) 函数,在接受到消息后会调用此函数, 用户可以在其中执行自己想要的操作.
4. Tuple: 一次消息传递的基本单元. 本来应该是一个 key-value 的 map, 但是由于各个组件间传递的tuple的字段名称已经事先定义好, 所以 tuple 中只要按序填入各个 value 就行了, 所以就是一个 value list.
5. Stream: 源源不断传递的 tuple 就组成了 Stream.

#### 2.1.2 Storm 架构

1. Nimbus: 主节点的守护进程, 负责为工作节点分发任务.
2. Supervisor: 负责接受 Nimbus 分配的任务, 启动和停止属于自己管理的 Worker 进程.
3. Worker: 运行具体处理组件逻辑的进程.
4. Task: Worker 中每一个 Spout/Bolt 的线程称为一个 Task. 在 Storm 0.8 之后, Task 不再与物理线程对应, 同一个 Spout/Bolt 的 Task 可能会共享一个物理线程, 该线程称为 executor.

下面这个图描述了以上几个角色之间的关系

![storm角色](storm角色.jpg "storm角色")

Storm 集群中包含两类节点: 主控节点(Master Node)和工作节点(Work Node). 其分别对应的角色如下:

* 主控节点(Master Node)上运行一个被称为 Nimbus 的后台程序, 它负责在 Storm 集群内分发代码, 分配任务给工作机器, 并且负责监控集群运行状态.
* 每个工作节点(Work Node)上运行一个被称为 Supervisor 的后台程序. Supervisor 负责监听从 Nimbus 分配给它执行的任务, 据此启动或停止执行任务的工作进程. 每一个工作进程执行一个 Topology 的子集(一部分); 一个运行中的 Topology 由分布在不同工作节点上的多个工作进程组成.

Nimbus 和 Supervisor 节点之间所有的协调工作是通过 Zookeeper 集群来实现的. 此外, Nimbus 和 Supervisor 进程都是快速失败(fail-fast)和无状态(stateless)的; Storm 集群所有的状态要么在 Zookeeper 集群中, 要么存储在本地磁盘上. 这意味着你可以用 kill -9 来杀死 Nimbus 和 Supervisor 进程, 它们在重启后可以继续工作. 这个设计使得Storm集群拥有不可思议的稳定性.

PS: 在 Storm 1.0 版本以前, Nimbus 会出现单点失效(概率比较小), 从 1.0 起, Nimbus 也可以组成集群了.

有两台 Nimbus: DEV236(非 Leader), DEV237(Leader)

![numbus集群](numbus集群.jpg "numbus集群")

Leader DEV237 挂掉(Offline)时, 另一台替上

![numbus集群的健壮性](numbus集群的健壮性.jpg "numbus集群的健壮性")

上图是通过 [Storm UI](# 2.2.2.6 启动 Storm 各个后台进程) 看到的

##### 2.2.2.2 搭建 Zookeeper 集群

见 [1.2.2 集群模式](# 1.2.2 集群模式)

关于 ZooKeeper 部署的两点说明:

* ZooKeeper 必须在监控模式下运行. 因为 ZooKeeper 是个快速失败系统, 如果遇到了故障, ZooKeeper 服务会主动关闭. 更多详细信息请参考: http://zookeeper.apache.org/doc/r3.3.3/zookeeperAdmin.html#sc_supervision. 我们选择第一个方案: daemontools: http://cr.yp.to/daemontools.html
* 需要设置一个 cron 服务来定时压缩 ZooKeeper 的数据与事务日志. 因为 ZooKeeper 的后台进程不会处理这个问题, 如果不配置 cron, ZooKeeper 的日志会很快填满磁盘空间. 更多详细信息请参考: http://zookeeper.apache.org/doc/r3.3.3/zookeeperAdmin.html#sc_maintenance
    * 代替方案: http://nileader.blog.51cto.com/1381108/932156 第四种
    > 在 zoo.cfg 中配置的:
    > autopurge.purgeInterval 这个参数指定了清理频率, 单位是小时, 需要填写一个 1 或更大的整数, 默认是 0, 表示不开启自己清理功能.
    > autopurge.snapRetainCount 这个参数和上面的参数搭配使用, 这个参数指定了需要保留的快照数目. 默认是保留 3 个.

### 2.4 Storm 常见问题

#### 2.4.1 配置问题

* 运行 storm 命令报错

出现语法错误:

```
File "/home/storm/apache-storm-0.9.3/bin/storm", line 61
   normclasspath = cygpath if sys.platform == 'cygwin' else identity
                            ^
SyntaxError: invalid syntax

```

这是由于系统中安装的低版本 Python 部分语法不支持, 需要重新安装高版本 Python(如2.7.x).

PS: 部分系统Python默认安装位置不是 `/usr/bin/python`, 必须在 Python 安装完成之后将安装版本Python关联到该位置. 参考操作方法: `cd /usr/bin` `mv python python.bk``ln -s /usr/local/Python-2.7.8/python python`

* Storm 在 ssh 断开后自动关闭

这是由于 Storm 是由默认的 Shell 机制打开运行, 在 ssh 或 telnet 断开后终端会将挂断信号发送到控制进程, 进而会关闭该 Shell 进程组中的所有进程. 因此需要在 Storm 后台启动时使用 `nohup` 命令和 `&` 标记可以使进程忽略挂断信号, 避免程序的异常退出:

```bash
nohup bin/storm nimbus > logs/nimbus-boot.log 2>&1 &
nohup bin/storm supervisor > logs/supervisor-boot.log 2>&1 &
nohup bin/storm ui > logs/ui-boot.log 2>&1 &
nohup bin/storm logviewer > logs/logviewer-boot.log 2>&1 &
```

* Storm UI 网页无法打开

检查 Storm 主机(nimbus 与 ui 所在运行服务器)的防火墙设置, 是否存在监控端口屏蔽(ui 的默认端口是 8080)

PS: 测试环境下可以不考虑安全问题直接关闭防火墙

* Strom UI 网页中没有 topology 信息

只有集群(Cluster)模式的 topology 才会在监控页面显示, 需要将提交到集群的 topology 的运行模式由本地模式(local mode)改为集群模式

* Storm UI 网页中无法打开各个端口的 worker.log

在需要查看 log 的机器上启动 logviewer 进程:

```bash
nohup bin/storm logviewer > logs/logviewer-boot.log 2>&1 &
```

* expected '<document start>', but found BlockMappingStart 错误

Storm 启动失败, 在 nohup.out 中有如下错误信息

```
Exception in thread "main" expected '<document start>', but found BlockMappingStart
```

一般在这类信息后会有相关错误位置说明信息, 如

```
in 'reader', line 23, column 2:
     nimbus.host: "hd124"
     ^
```

或者

```
in 'reader', line 7, column 1:
     storm.zookeeper.port: 2181
     ^
```

这类错误主要是storm.yaml文件的配置格式错误造成的, 一般是配置项的空格遗漏问题. 如上面两例分别表示nimbus.host与storm.zookeeper.port两个配置项开头缺少空格, 或者":"后缺少空格. 正确添加空格后重新启动Storm即可.

* Storm worker 数量与配置数量不一致

在 topology 中设置 worker 数量:

`conf.setNumWorkers(6);`

但是, 集群中实际的 worker 数量却不到6.

这是由于每个 supervisor 中有 worker 数量的上限, 这个上限值除了要满足系统允许的最大 slot 上限值 `8` 之外, 还需要小于 Storm 配置文件中的端口数量:

```
supervisor.slots.ports:
    - 6700
    - 6701
    - 6702
    - 6703
```

例如这里 supervisor 只配置了 4 个端口, 那么在这个 supervisor 上最多只能运行 4 个 worker 进程. 因此, 如果需要更多的 worker 就需要配置更多的端口.

* 日志无法记录到程序中配置的路径

Storm 默认将日志统一记录到 `$STORM_HOME/logs` 目录中, 不支持在程序中自定义的路径. 但是, 集群的日志记录目录是可以修改的, 0.9 以上版本的 Storm 可以在 `$STORM_HOME/logback/cluster.xml` 配置文件中修改, 其他早期版本可以在`log4j/*.properties` 配置文件中修改.

#### 2.4.2 开发问题

* log4j 包冲突

传统的日志记录方法是如下所示引入 apache 的 log4j 包来记录日志

```
import org.apache.log4j.Logger;
public class ClassifyBolt {
    private static final Logger LOG = Logger.getLogger(ClassifyBolt.class);
}
```

这种方式在 Storm 开发中会报包冲突错误

```
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/F:/maven/repository/ch/qos/logback/logback-classic/1.0.13/logback-classic-1.0.13.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/F:/maven/repository/org/slf4j/slf4j-log4j12/1.6.1/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [ch.qos.logback.classic.util.ContextSelectorStaticBinder]
SLF4J: Detected both log4j-over-slf4j.jar AND slf4j-log4j12.jar on the class path, preempting StackOverflowError.
SLF4J: See also http://www.slf4j.org/codes.html#log4jDelegationLoop for more details.
Exception in thread "main" java.lang.ExceptionInInitializerError
    at org.apache.log4j.Logger.getLogger(Logger.java:39)
    at org.apache.log4j.Logger.getLogger(Logger.java:43)
    at com.enjoyor.storm.estimation.bolt.ClassifyBolt.<clinit>(ClassifyBolt.java:25)
    at com.enjoyor.storm.estimation.topology.SimulationTopology.main(SimulationTopology.java:153)
Caused by: java.lang.IllegalStateException: Detected both log4j-over-slf4j.jar AND slf4j-log4j12.jar on the class path, preempting StackOverflowError. See also http://www.slf4j.org/codes.html#log4jDelegationLoop for more details.
    at org.apache.log4j.Log4jLoggerFactory.<clinit>(Log4jLoggerFactory.java:49)
    ... 4 more
```

解决方法是改变日志类处理方式, 替换 apache 的依赖为 slf4j 原生包, 如下所示

```
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public class ClassifyBolt {
    private static final Logger LOG = LoggerFactory
            .getLogger(ClassifyBolt.class);
}
```

* 部件命名重复错误

对于具有相似功能的 Bolt/Spout 可能会出现命名冲突问题, 例如这里定义了三个中介者 Bolt:

```
String preInter = "medium";
String detInter = "medium";
String devInter = "medium";

builder.setBolt(preInter, new InterBolt().shuffleGrouping(pre); builder.setBolt(detInter, new InterBolt().shuffleGrouping(det);
builder.setBolt(devInter, new InterBolt().shuffleGrouping(dev);
```

虽然他们的变量名不同, 但是实际的字符串对象名称是相同的(都是"medium"), 这就会产生如下的非法参数错误

```
Exception in thread "main" java.lang.IllegalArgumentException: Bolt has already been declared for id medium
    at backtype.storm.topology.TopologyBuilder.validateUnusedId(TopologyBuilder.java:212)
    at backtype.storm.topology.TopologyBuilder.setBolt(TopologyBuilder.java:139)
    at com.enjoyor.storm.estimation.topology.SimulationTopology.main(SimulationTopology.java:182)
```

所以, 对于不同的部件(Spout/Bolt), 务必要区别命名:

```
String preInter = "premedium";
String detInter = "detmedium";
String devInter = "devmedium";
```

这样就能保证部件的ID互不相同, 就能够避免错误了.

* 下游Bolt未定义数据流错误

在下游Bolt接收数据时, 往往会忽略具体的接收数据流名称, 例如

```
builder.setBolt(devInter, new InterBolt().shuffleGrouping(dev);
```

这里的 grouping 过程就忽略了"dev"的数据流ID(streamId), 在运行时会报错

```
3839 [main] WARN  backtype.storm.daemon.nimbus - Topology submission exception. (topology name='simulation') #<InvalidTopologyException InvalidTopologyException(msg:Component: [devInter] subscribes from non-existent stream: [default] of component [dev])>
7119 [main] ERROR org.apache.storm.zookeeper.server.NIOServerCnxnFactory - Thread Thread[main,5,main] died
```

因为不定义数据流时Spout/Bolt 会默认发送/接收streamId为"default"的数据流, 而当上游Bolt发送了包含自定义数据流ID的数据流时, 下游Bolt就无法识别, 所以此时需要在下游Bolt中定义数据流

```
builder.setBolt(devInter, new InterBolt().shuffleGrouping(dev, signalStream);
```

这里的"signalStream"就是上游Bolt发送的具体数据流名称.

## 3 Kafka

### 3.1 简介

[Apache Kafka](http://kafka.apache.org/)是一个分布式消息发布订阅系统. Kafka 系统快速、可扩展并且可持久化. 它的分区特性, 可复制和可容错都是其不错的特性.

#### 3.1.1 基本概念

Apache Kafka 与传统消息系统相比, 有以下不同:

* 它被设计为一个分布式系统, 易于向外扩展;
* 它同时为发布和订阅提供高吞吐量;
* 它支持多订阅者, 当失败时能自动平衡消费者;
* 它将消息持久化到磁盘, 因此可用于批量消费, 例如 ETL, 以及实时应用程序.

首先来了解一下 Kafka 所使用的基本术语:

* Topic: Kafka 将消息种子(Feed)分门别类, 每一类的消息称之为话题(Topic).
* Producer: 发布消息的对象称之为话题生产者(Topic Producer)
* Consumer: 订阅消息并处理发布的消息的种子的对象称之为话题消费者(Topic Consumers)
* Broker: 已发布的消息保存在一组服务器中, 称之为 Kafka 集群. 集群中的每一个服务器都是一个代理(Broker). 消费者可以订阅一个或多个话题, 并从 Broker 拉数据, 从而消费这些已发布的消息.

听起来和 JMS 消息处理差不多?

让我们站的高一点, 从高的角度来看, Kafka 集群的业务处理就像这样子:

![Kafka集群](Kafka集群.png "Kafka集群")

* 多个 Broker 协同合作, Producer、Consumer 和 Broker 三者之间通过 zookeeper 来协调请求和转发.
* Producer 产生和推送(push)数据到 Broker, Consumer 从 Broker 拉取(pull)数据并进行处理.
* Broker 端不维护数据的消费状态, 提升了性能.
* 直接使用磁盘进行存储, 线性读写, 速度快: 避免了数据在 JVM 内存和系统内存之间的复制, 减少耗性能的创建对象和垃圾回收.
* Client 和 Server 之间的交流通过一条简单、高性能并且不局限某种开发语言的 TCP 协议. 除了Java Client 外, 还有非常多的其它编程语言的[Client](https://cwiki.apache.org/confluence/display/KAFKA/Clients).

#### 3.1.2 话题和日志 (Topic 和 Log)

更深入的了解一下 Kafka 中的 Topic.
Topic 是发布的消息的类别或者种子 Feed 名. 对于每一个 Topic, Kafka 集群维护这一个分区的 log, 就像下图中的示例:

![Kafka话题](Kafka话题.png "Kafka话题")

每一个分区都是一个顺序的、不可变的消息队列, 并且可以持续的添加. 分区中的消息都被分配了一个序列号, 称之为偏移量(offset), 在每个分区中此偏移量都是唯一的.

Kafka集群保持所有的消息, 直到它们过期, 无论消息是否被消费了.

实际上消费者所持有的仅有的元数据就是这个偏移量, 也就是消费者在这个 log 中的位置. 这个偏移量由消费者控制: 正常情况当消费者消费消息的时候, 偏移量也线性的的增加. 但是实际偏移量由消费者控制, 消费者可以将偏移量重置为更老的一个偏移量, 重新读取消息.

可以看到这种设计对消费者来说操作自如, 一个消费者的操作不会影响其它消费者对此 log 的处理.

再说说分区. Kafka 中采用分区的设计有几个目的. 一是可以处理更多的消息, 不受单台服务器的限制. Topic 拥有多个分区意味着它可以不受限的处理更多的数据. 第二, 分区可以作为并行处理的单元, 稍后会谈到这一点.

#### 3.1.3 分布式(Distribution)

Log 的分区被分布到集群中的多个服务器上. 每个服务器处理它分到的分区. 根据配置每个分区还可以复制到其它服务器作为备份容错.

每个分区有一个 leader, 零或多个 follower. Leader 处理此分区的所有的读写请求而 follower 被动的复制数据. 如果 leader 当机, 其它的一个 follower 会被推举为新的 leader.

一台服务器可能同时是一个分区的 leader, 另一个分区的 follower. 这样可以平衡负载, 避免所有的请求都只让一台或者某几台服务器处理.

#### 3.1.4 生产者(Producers)

生产者往某个 Topic 上发布消息. 生产者也负责选择发布到这此 Topic 上的哪一个分区. 最简单的方式从分区列表中轮流选择. 也可以根据某种算法依照权重选择分区. 开发者负责如何选择分区的算法.

#### 3.1.5 消费者(Consumers)

通常来讲, 消息模型可以分为两种, 队列和发布-订阅式. 队列的处理方式是 一组消费者从服务器读取消息, 一条消息只有其中的一个消费者来处理. 在发布-订阅模型中, 消息被广播给所有的消费者, 接收到消息的消费者都可以处理此消息. Kafka 为这两种模型提供了单一的消费者抽象模型: 消费者组(Consumer group).

消费者用一个消费者组名标记自己. 一个发布在 Topic 上消息被分发给此消费者组中的一个消费者.

* 假如所有的消费者都在一个组中, 那么这就变成了 **queue 模型**.
* 假如所有的消费者都在不同的组中, 那么就完全变成了 **发布-订阅模型**.

更通用的, 我们可以创建一些消费者组作为逻辑上的订阅者. 每个组包含数目不等的消费者, 一个组内多个消费者可以用来扩展性能和容错. 正如下图所示:

![Kafka消费者](Kafka消费者.png "Kafka消费者")

2 server Kafka 集群 维持 4 partitions (P0-P3) 并有 2 个 Consumer 组. Consumer 组 A 有 2 个消费者, 组 B 有 4 个

正像传统的消息系统一样, Kafka 保证消息的顺序不变.

再详细扯几句. 传统的队列模型保持消息, 并且保证它们的先后顺序不变. 但是, 尽管服务器保证了消息的顺序, 消息还是异步的发送给各个消费者, 消费者收到消息的先后顺序不能保证了. 这也意味着并行消费将不能保证消息的先后顺序. 用过传统的消息系统的同学肯定清楚, 消息的顺序处理很让人头痛. 如果只让一个消费者处理消息, 又违背了并行处理的初衷.

在这一点上 Kafka 做的更好, 尽管并没有完全解决上述问题. Kafka 采用了一种分而治之的策略: 分区. 因为 Topic 分区中消息只能由消费者组中的唯一一个消费者处理, 所以消息肯定是按照先后顺序进行处理的. 但是它也仅仅是保证 Topic 的一个分区顺序处理, 不能保证跨分区的消息先后处理顺序.

所以, 如果你想要顺序的处理 Topic 的所有消息, 那就只提供一个分区.

#### 3.1.6 Kafka 的保证(Guarantees)

* 生产者发送到一个特定的 Topic 的分区上的消息将会按照它们发送的顺序依次加入
* 消费者收到的消息也是此顺序
* 如果一个 Topic 配置了复制因子( replication facto)为 N, 那么可以允许 N-1 服务器当掉而不丢失任何已经增加的消息

#### 3.1.7 用例 (Use CASE)

Kafka 可以用于:

* 消息系统, 例如 ActiveMQ 和 RabbitMQ.
* 站点的用户活动追踪. 用来记录用户的页面浏览, 搜索, 点击等.
* 操作审计. 用户/管理员的网站操作的监控.
* 日志聚合. 收集数据, 集中处理.
* 流处理.
* [Event sourcing](http://martinfowler.com/eaaDev/EventSourcing.html)
* Commit Log

```
########################## Topic Basics ##########################
delete.topic.enable=true       # 配置为可以使用 delete topic 命令


###################### Log Retention Policy ######################
log.roll.hours=2               # 开始一个新的 log 文件片段的最大时间
log.retention.hours=24         # 控制一个 log 文件保留多长个小时
log.retention.bytes=1073741824 # 所有 log 文件的最大大小
log.segment.bytes=104857600    # 单一的 log 文件最大大小
log.cleanup.policy=delete      # log 清除策略
log.retention.check.interval.ms=60000
```

## 4 最后

* 因为上面这三个框架都是 Java 的, 所以可以调整 Java 堆大小以优化上面这些程序的运行. Java 堆太小会导致程序难以运行; Java 堆太大(超出物理内存)会导致程序被交换到磁盘, 性能急剧降低. 例如: 4 G 内存的专用服务器可以分配 3 G 的 Java 堆, 最好的建议是运行负载测试, 然后确保远低于会导致系统交换的堆大小.
